{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Libraries and Define Constants\n",
    "\n",
    "In this section, we import necessary libraries (`pandas` and `os`) and define constants that will be used throughout the notebook. These constants include paths to data files, directory paths for outputs, and configuration data for various utilities like electricity, gas, water, etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "519e14f39ee2bf02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imports and Configuration Constants\n",
    "import pandas as pd\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T16:48:37.629461Z",
     "start_time": "2024-11-06T16:48:37.253043Z"
    }
   },
   "id": "fcacb4dc27b61388",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Define dataset names and configurations for each dataset type\n",
    "DATASET_NAMES = [\n",
    "    \"electricity\",\n",
    "    \"gas\",\n",
    "    \"solar\",\n",
    "    \"steam\",\n",
    "    \"water\",\n",
    "]\n",
    "data_map = {\n",
    "    \"electricity\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Mouse\",\n",
    "            \"building_name\": \"science\",\n",
    "            \"consumer_name\": \"Micheal\",\n",
    "        },\n",
    "        1: {\"site_name\": \"Mouse\", \"building_name\": \"health\", \"consumer_name\": \"Estela\"},\n",
    "    },\n",
    "    \"gas\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Mohammad\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"lodging\",\n",
    "            \"consumer_name\": \"Dean\",\n",
    "        },\n",
    "    },\n",
    "    \"solar\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Bobcat\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Alissa\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Bobcat\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Coleman\",\n",
    "        },\n",
    "    },\n",
    "    \"water\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"lodging\",\n",
    "            \"consumer_name\": \"Cora\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Wolf\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Ursula\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "TARGET_FILE_TEMPLATE = \"../data/meters/cleaned/{}_cleaned.csv\"\n",
    "WEATHER_FILE = \"../data/weather/weather.csv\"\n",
    "OUTPUT_DIR = \"../data/meters/final/\"\n",
    "TARGET_FILE_ELECTRICITY = \"../data/ELECTRICITY/ELECTRICITY.txt\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T16:48:37.634094Z",
     "start_time": "2024-11-06T16:48:37.630511Z"
    }
   },
   "id": "60c2b30582fbe87c",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions for Data Processing\n",
    "\n",
    "Here, we define two core functions: `extract_and_merge_data` for data extraction, merging, and feature aggregation, and `split_train_test` for splitting the dataset into training and testing periods.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6e552b459261fd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `extract_and_merge_data`\n",
    "\n",
    "This function loads and merges consumption and weather data for a given utility dataset, resamples the data to a daily frequency, and calculates daily aggregate statistics. The final processed dataset is saved as a CSV file.\n",
    "\n",
    "### Parameters:\n",
    "- `dataset_name` (str): Utility dataset name.\n",
    "- `config` (dict): Configuration dictionary containing site, building, and consumer names.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "481e0ead518ad333"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_and_merge_data(dataset_name, config):\n",
    "    site_name = config[\"site_name\"]\n",
    "    building_name = config[\"building_name\"]\n",
    "    consumer_name = config[\"consumer_name\"]\n",
    "\n",
    "    # Define file paths and target/output columns\n",
    "    target_file = TARGET_FILE_TEMPLATE.format(dataset_name)\n",
    "    processed_file = f\"{dataset_name}_{site_name}_{building_name}_{consumer_name}\"\n",
    "    output_file = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        processed_file.upper(),\n",
    "        f\"{processed_file}.csv\".upper(),\n",
    "    )\n",
    "\n",
    "    target_column = f\"{site_name}_{building_name}_{consumer_name}\"\n",
    "    new_target_column = dataset_name.capitalize()\n",
    "\n",
    "    try:\n",
    "        # Load target data (meter readings)\n",
    "        target_df = pd.read_csv(target_file, usecols=[\"timestamp\", target_column])\n",
    "        target_df = target_df.rename(columns={target_column: new_target_column})\n",
    "        target_df[\"timestamp\"] = pd.to_datetime(target_df[\"timestamp\"])\n",
    "\n",
    "        # Load weather data, filter for the specific site, and drop `site_id`\n",
    "        weather_df = pd.read_csv(WEATHER_FILE)\n",
    "        weather_df = weather_df[weather_df[\"site_id\"] == site_name].drop(\n",
    "            columns=[\"site_id\"]\n",
    "        )\n",
    "        weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "\n",
    "        # Merge on timestamp, keeping timestamps from the target data only\n",
    "        merged_df = pd.merge(target_df, weather_df, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "        # Resample the data to daily frequency using sum for consumption and mean for weather data\n",
    "        resampled_df = merged_df.resample(\"D\", on=\"timestamp\").agg(\n",
    "            {\n",
    "                new_target_column: [\n",
    "                    \"sum\",\n",
    "                    \"mean\",\n",
    "                    \"min\",\n",
    "                    \"max\",\n",
    "                    \"first\",\n",
    "                    \"last\",\n",
    "                    \"median\",\n",
    "                ],\n",
    "                **{col: \"mean\" for col in weather_df.columns if col != \"timestamp\"},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Flatten column names after aggregation\n",
    "        resampled_df.columns = [\n",
    "            \"sum_conso\",\n",
    "            \"mean_conso\",\n",
    "            \"min_conso\",\n",
    "            \"max_conso\",\n",
    "            \"first_conso\",\n",
    "            \"last_conso\",\n",
    "            \"median_conso\",\n",
    "        ] + list(weather_df.columns[1:])\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "\n",
    "        # Add time-related features\n",
    "        resampled_df[\"month\"] = resampled_df[\"timestamp\"].dt.month\n",
    "        resampled_df[\"day_of_week\"] = resampled_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "        # Save to the specified output file without removing `timestamp`\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        resampled_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Data extracted and saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"File not found: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\n",
    "            f\"Column error: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T16:48:37.641119Z",
     "start_time": "2024-11-06T16:48:37.635176Z"
    }
   },
   "id": "18319866860a7106",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `split_train_test`\n",
    "\n",
    "Splits the processed dataset into training and testing sets based on the year of data. Training data consists of 2016 records, and testing data consists of 2017 records.\n",
    "\n",
    "### Parameters:\n",
    "- `output_file` (str): Path to the processed dataset file.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "699fc41d74612468"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_train_test(output_file):\n",
    "    if output_file is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.read_csv(output_file, parse_dates=[\"timestamp\"])\n",
    "        train_df = merged_df[merged_df[\"timestamp\"].dt.year == 2016]\n",
    "        test_df = merged_df[merged_df[\"timestamp\"].dt.year == 2017]\n",
    "\n",
    "        train_file = output_file.replace(\".CSV\", \"_TRAIN.CSV\")\n",
    "        test_file = output_file.replace(\".CSV\", \"_TEST.CSV\")\n",
    "\n",
    "        train_df.drop(columns=[\"timestamp\"]).to_csv(train_file, index=False)\n",
    "        test_df.drop(columns=[\"timestamp\"]).to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"Train data saved to {train_file}\")\n",
    "        print(f\"Test data saved to {test_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during train-test split: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T16:48:37.647458Z",
     "start_time": "2024-11-06T16:48:37.643751Z"
    }
   },
   "id": "2a4d94cfd55793a2",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execution Loop for Data Processing\n",
    "\n",
    "This section iterates over each dataset in `DATASET_NAMES` and each configuration in `data_map`, calling `extract_and_merge_data` for data extraction and merging, followed by `split_train_test` for train-test splitting.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f56f40f2579e91"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD.CSV\n",
      "Train data saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN.CSV\n",
      "Train data saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA.CSV\n",
      "Train data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN.CSV\n",
      "Train data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA.CSV\n",
      "Train data saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA.CSV\n",
      "Train data saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA_TEST.CSV\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in DATASET_NAMES:\n",
    "    if dataset_name in data_map:\n",
    "        for config in data_map[dataset_name].values():\n",
    "            output_file = extract_and_merge_data(dataset_name, config)\n",
    "            split_train_test(output_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T16:48:40.673647Z",
     "start_time": "2024-11-06T16:48:37.648419Z"
    }
   },
   "id": "43156a88fe90bb68",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process Electricity Data\n",
    "\n",
    "The `process_electricity_data` function loads a separate electricity dataset with specific aggregations and saves it in the specified output directory. The `split_train_test` function then separates this data into training and testing sets.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "916e27f9576d61b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `process_electricity_data`\n",
    "\n",
    "Processes electricity data, resamples it to daily frequency, and calculates aggregation statistics. The result is saved in the specified output file.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b09a2c6a2e31e91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def process_electricity_data():\n",
    "    # Define file path and output file name\n",
    "    processed_file = \"electricity_global_reactive_power\"\n",
    "    output_file = os.path.join(\n",
    "        OUTPUT_DIR, processed_file.upper(), f\"{processed_file}.csv\".upper()\n",
    "    )\n",
    "\n",
    "    # Load the electricity data with Date and Time combined into a timestamp\n",
    "    try:\n",
    "        electricity_df = pd.read_csv(\n",
    "            TARGET_FILE_ELECTRICITY,\n",
    "            sep=\";\",  # Separator is ';'\n",
    "            parse_dates=[[0, 1]],  # Combine 'Date' and 'Time' columns\n",
    "            dayfirst=True,  # Use day-first format for dates\n",
    "            na_values=\"?\",  # Handle missing values\n",
    "        )\n",
    "        electricity_df.columns = [\n",
    "            \"timestamp\",\n",
    "            \"Global_active_power\",\n",
    "            \"Global_reactive_power\",\n",
    "            \"Voltage\",\n",
    "            \"Global_intensity\",\n",
    "            \"Sub_metering_1\",\n",
    "            \"Sub_metering_2\",\n",
    "            \"Sub_metering_3\",\n",
    "        ]\n",
    "        electricity_df[\"timestamp\"] = pd.to_datetime(\n",
    "            electricity_df[\"timestamp\"], errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # Drop rows with invalid dates\n",
    "        electricity_df.dropna(subset=[\"timestamp\"], inplace=True)\n",
    "\n",
    "        # Define detailed aggregations for `Global_reactive_power`\n",
    "        aggregation_dict = {\n",
    "            \"Global_reactive_power\": [\n",
    "                \"sum\",\n",
    "                \"mean\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"first\",\n",
    "                \"last\",\n",
    "                \"median\",\n",
    "            ]  # Detailed aggregations\n",
    "        }\n",
    "\n",
    "        # Simple sum aggregation for other columns\n",
    "        other_columns = {\n",
    "            \"Voltage\": \"sum\",\n",
    "            \"Global_intensity\": \"sum\",\n",
    "            \"Sub_metering_1\": \"sum\",\n",
    "            \"Sub_metering_2\": \"sum\",\n",
    "            \"Sub_metering_3\": \"sum\",\n",
    "        }\n",
    "\n",
    "        # Combine all aggregations into a single dictionary\n",
    "        aggregation_dict.update(other_columns)\n",
    "\n",
    "        # Resample the data to daily frequency using the specified aggregations\n",
    "        resampled_df = electricity_df.resample(\"D\", on=\"timestamp\").agg(\n",
    "            aggregation_dict\n",
    "        )\n",
    "\n",
    "        # Rename columns for `Global_reactive_power` aggregations only\n",
    "        resampled_df.columns = [\n",
    "            \"sum_conso\",\n",
    "            \"mean_conso\",\n",
    "            \"min_conso\",\n",
    "            \"max_conso\",\n",
    "            \"first_conso\",\n",
    "            \"last_conso\",\n",
    "            \"median_conso\",\n",
    "        ] + list(other_columns.keys())\n",
    "\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "\n",
    "        # Add time-related features\n",
    "        resampled_df[\"month\"] = resampled_df[\"timestamp\"].dt.month\n",
    "        resampled_df[\"day_of_week\"] = resampled_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "        # Convert all column names to lowercase\n",
    "        resampled_df.columns = [col.lower() for col in resampled_df.columns]\n",
    "\n",
    "        # Save to the specified output file\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        resampled_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Electricity data processed and saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}. Skipping electricity data.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Value error: {e}. Skipping electricity data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Skipping electricity data.\")\n",
    "\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T18:09:27.894035Z",
     "start_time": "2024-11-06T18:09:27.881696Z"
    }
   },
   "id": "68db0857289450e3",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Electricity Data Processing\n",
    "\n",
    "Finally, we call `process_electricity_data` and `split_train_test` to handle the electricity dataset specifically.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "393c99028eae3d60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_22248/4062433383.py:10: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  electricity_df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electricity data processed and saved to ../data/meters/final/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER_TEST.CSV\n"
     ]
    }
   ],
   "source": [
    "output_file = process_electricity_data()\n",
    "split_train_test(output_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T18:09:37.201757Z",
     "start_time": "2024-11-06T18:09:33.595949Z"
    }
   },
   "id": "9c15a893c7b828d5",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print out data names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4470fb8720577f4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_data_paths = [\n",
      "    \"../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD.csv\",\n",
      "    \"../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA.csv\",\n",
      "    \"../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA.csv\",\n",
      "    \"../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA.csv\",\n",
      "    \"../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN.csv\",\n",
      "    \"../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA.csv\",\n",
      "    \"../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL.csv\",\n",
      "    \"../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO.csv\",\n",
      "    \"../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA.csv\",\n",
      "    \"../data/meters/final/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER.csv\",\n",
      "    \"../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN.csv\",\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the processed data directories\n",
    "base_directory = \"../data/meters/final\"\n",
    "\n",
    "# Generate a list of paths to each main .csv file in subdirectories, excluding \"TRAIN\" and \"TEST\"\n",
    "processed_data_paths = []\n",
    "for subdirectory in os.listdir(base_directory):\n",
    "    subdirectory_path = os.path.join(base_directory, subdirectory)\n",
    "    # Check if itâ€™s a directory and contains a .csv file without \"TRAIN\" or \"TEST\"\n",
    "    if os.path.isdir(subdirectory_path):\n",
    "        csv_filename = f\"{subdirectory}.csv\"\n",
    "        csv_filepath = os.path.join(subdirectory_path, csv_filename)\n",
    "        if os.path.isfile(csv_filepath) and \"TRAIN\" not in csv_filename and \"TEST\" not in csv_filename:\n",
    "            # Add the file path, replacing backslashes with forward slashes for consistency\n",
    "            processed_data_paths.append(csv_filepath.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# Print the list in the desired format\n",
    "print(\"processed_data_paths = [\")\n",
    "for path in processed_data_paths:\n",
    "    print(f'    \"{path}\",')\n",
    "print(\"]\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T17:38:54.117143Z",
     "start_time": "2024-11-06T17:38:54.105905Z"
    }
   },
   "id": "e6516869a85f95a8",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "291043b1ffce8cd1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
