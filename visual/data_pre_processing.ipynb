{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Aggregation and Feature Extraction\n",
    "\n",
    "This script processes multiple datasets for various utilities (e.g., chilled water, electricity, gas), merging time series data for each and performing daily resampling and aggregation. After merging, additional features are calculated, and train/test datasets are generated for modeling or further analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The script performs the following tasks for each dataset:\n",
    "\n",
    "1. **Merge Data**: Combines consumption data with weather data.\n",
    "2. **Resample and Aggregate**: Resamples to daily frequency, calculates daily aggregates, and extracts time-related features.\n",
    "3. **Split into Train and Test Sets**: Splits the data into 2016 (train) and 2017 (test) datasets.\n",
    "\n",
    "## Code Details\n",
    "\n",
    "### Global Variables\n",
    "\n",
    "- `DATASET_NAMES`: List of utility datasets to process, such as \"chilledwater\" and \"electricity\".\n",
    "- `data_map`: Dictionary containing the site, building, and consumer configurations for each dataset type.\n",
    "- `TARGET_FILE_TEMPLATE`: Template path for the meter data file for each dataset.\n",
    "- `WEATHER_FILE`: Path for the weather data file.\n",
    "- `OUTPUT_DIR`: Directory where the final processed files will be saved.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### 1. `extract_and_merge_data(dataset_name, config)`\n",
    "\n",
    "This function merges the meter and weather data, performs daily resampling, calculates aggregation-based features, and saves the results.\n",
    "\n",
    "- **Parameters**:\n",
    "\n",
    "  - `dataset_name` (str): Name of the dataset (e.g., \"water\").\n",
    "  - `config` (dict): Configuration containing `site_name`, `building_name`, and `consumer_name`.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  1. **File Paths and Columns**: Constructs paths and target columns based on dataset configuration.\n",
    "     - Output paths are organized in subfolders using `dataset_name`, `site_name`, `building_name`, and `consumer_name`, so each file has a dedicated folder.\n",
    "  2. **Data Loading**: Reads the meter and weather data, ensuring only relevant columns are loaded.\n",
    "  3. **Merging**: Merges data on `timestamp` and keeps only relevant timestamps.\n",
    "  4. **Daily Resampling**:\n",
    "     - Resamples on daily frequency (`'D'`) using `sum` for the primary target column and `mean` for weather data.\n",
    "     - Calculates the following features for each day:\n",
    "       - `sum_conso`: Daily sum of consumption.\n",
    "       - `mean_conso`: Daily mean of consumption.\n",
    "       - `min_conso`: Daily minimum consumption.\n",
    "       - `max_conso`: Daily maximum consumption.\n",
    "       - `first_conso`: First recorded consumption value for the day.\n",
    "       - `last_conso`: Last recorded consumption value for the day.\n",
    "       - `median_conso`: Median consumption for the day.\n",
    "       - `month`: Month extracted from `timestamp`.\n",
    "       - `day_of_week`: Day of the week (Monday = 0, Sunday = 6).\n",
    "  5. **Remove Timestamp Column**: After resampling and feature generation, the `timestamp` column is removed from the final processed file.\n",
    "  6. **Error Handling**: Catches and logs any issues with file reading, column mismatches, or merging issues.\n",
    "\n",
    "- **Returns**:\n",
    "  - `output_file` (str): Path to the saved file with the resampled and aggregated data, or `None` if an error occurs.\n",
    "\n",
    "#### 2. `split_train_test(output_file)`\n",
    "\n",
    "This function splits the resampled dataset into train (2016) and test (2017) datasets.\n",
    "\n",
    "- **Parameters**:\n",
    "\n",
    "  - `output_file` (str): Path to the file generated by `extract_and_merge_data`.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  1. **Load Data**: Reads the `output_file`.\n",
    "  2. **Train/Test Split**:\n",
    "     - Filters rows with `timestamp` in 2016 as the train set.\n",
    "     - Filters rows with `timestamp` in 2017 as the test set.\n",
    "  3. **Save Train and Test Files**:\n",
    "     - Saves the train set as `_TRAIN.CSV` and the test set as `_TEST.CSV` in their respective folders.\n",
    "\n",
    "- **Error Handling**: Catches any issues during the loading or filtering process and logs an error.\n",
    "\n",
    "### Main Execution Loop\n",
    "\n",
    "Iterates over each dataset in `DATASET_NAMES` and each configuration in `data_map`:\n",
    "\n",
    "1. Calls `extract_and_merge_data` to process and save the aggregated data.\n",
    "2. Calls `split_train_test` to split the aggregated data into train and test sets.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "# Example to process each dataset and configuration\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    if dataset_name in data_map:\n",
    "        for index, config in data_map[dataset_name].items():\n",
    "            output_file = extract_and_merge_data(dataset_name, config)\n",
    "            split_train_test(output_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T18:42:23.532962Z",
     "start_time": "2024-11-06T18:42:19.968048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD.CSV\n",
      "Train data saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN.CSV\n",
      "Train data saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA.CSV\n",
      "Train data saved to ../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO.CSV\n",
      "Train data saved to ../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA.CSV\n",
      "Train data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN.CSV\n",
      "Train data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA.CSV\n",
      "Train data saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA.CSV\n",
      "Train data saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA_TEST.CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define dataset names and data map with configurations for each dataset type\n",
    "DATASET_NAMES = [\n",
    "    \"electricity\",\n",
    "    \"gas\",\n",
    "    \"hotwater\",\n",
    "    \"solar\",\n",
    "    \"water\",\n",
    "]\n",
    "data_map = {\n",
    "    \"electricity\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Mouse\",\n",
    "            \"building_name\": \"science\",\n",
    "            \"consumer_name\": \"Micheal\",\n",
    "        },\n",
    "        1: {\"site_name\": \"Mouse\", \"building_name\": \"health\", \"consumer_name\": \"Estela\"},\n",
    "    },\n",
    "    \"gas\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Mohammad\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"lodging\",\n",
    "            \"consumer_name\": \"Dean\",\n",
    "        },\n",
    "    },\n",
    "    \"hotwater\": {\n",
    "        0: {\"site_name\": \"Fox\", \"building_name\": \"lodging\", \"consumer_name\": \"Alana\"},\n",
    "        1: {\n",
    "            \"site_name\": \"Robin\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Margarito\",\n",
    "        },\n",
    "    },\n",
    "    \"solar\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Bobcat\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Alissa\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Bobcat\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Coleman\",\n",
    "        },\n",
    "    },\n",
    "    \"water\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"lodging\",\n",
    "            \"consumer_name\": \"Cora\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Wolf\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Ursula\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "TARGET_FILE_TEMPLATE = \"../data/meters/cleaned/{}_cleaned.csv\"\n",
    "WEATHER_FILE = \"../data/weather/weather.csv\"\n",
    "OUTPUT_DIR = \"../data/meters/final/\"\n",
    "\n",
    "\n",
    "def extract_and_merge_data(dataset_name, config):\n",
    "    site_name = config[\"site_name\"]\n",
    "    building_name = config[\"building_name\"]\n",
    "    consumer_name = config[\"consumer_name\"]\n",
    "\n",
    "    # Define file paths and target/output columns\n",
    "    target_file = TARGET_FILE_TEMPLATE.format(dataset_name)\n",
    "    processed_file = f\"{dataset_name}_{site_name}_{building_name}_{consumer_name}\"\n",
    "    output_file = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        processed_file.upper(),\n",
    "        f\"{processed_file}.csv\".upper(),\n",
    "    )\n",
    "\n",
    "    target_column = f\"{site_name}_{building_name}_{consumer_name}\"\n",
    "    new_target_column = dataset_name.capitalize()\n",
    "\n",
    "    try:\n",
    "        # Load target data (meter readings)\n",
    "        target_df = pd.read_csv(target_file, usecols=[\"timestamp\", target_column])\n",
    "        target_df = target_df.rename(columns={target_column: new_target_column})\n",
    "        target_df[\"timestamp\"] = pd.to_datetime(target_df[\"timestamp\"])\n",
    "\n",
    "        # Load weather data, filter for the specific site, and drop `site_id`\n",
    "        weather_df = pd.read_csv(WEATHER_FILE)\n",
    "        weather_df = weather_df[weather_df[\"site_id\"] == site_name].drop(\n",
    "            columns=[\"site_id\"]\n",
    "        )\n",
    "        weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "\n",
    "        # Merge on timestamp, keeping timestamps from the target data only\n",
    "        merged_df = pd.merge(target_df, weather_df, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "        # Resample the data to daily frequency using sum for consumption and mean for weather data\n",
    "        resampled_df = merged_df.resample(\"D\", on=\"timestamp\").agg(\n",
    "            {\n",
    "                new_target_column: [\n",
    "                    \"sum\",\n",
    "                    \"mean\",\n",
    "                    \"min\",\n",
    "                    \"max\",\n",
    "                    \"first\",\n",
    "                    \"last\",\n",
    "                    \"median\",\n",
    "                ],\n",
    "                **{col: \"mean\" for col in weather_df.columns if col != \"timestamp\"},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Flatten column names after aggregation\n",
    "        resampled_df.columns = [\n",
    "            \"sum_conso\",\n",
    "            \"mean_conso\",\n",
    "            \"min_conso\",\n",
    "            \"max_conso\",\n",
    "            \"first_conso\",\n",
    "            \"last_conso\",\n",
    "            \"median_conso\",\n",
    "        ] + list(weather_df.columns[1:])\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "\n",
    "        # Add time-related features\n",
    "        resampled_df[\"month\"] = resampled_df[\"timestamp\"].dt.month\n",
    "        resampled_df[\"day_of_week\"] = resampled_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "        # Save to the specified output file without removing `timestamp`\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        resampled_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Data extracted and saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"File not found: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\n",
    "            f\"Column error: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_train_test(output_file):\n",
    "    if output_file is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the merged data with `timestamp` included\n",
    "        merged_df = pd.read_csv(output_file, parse_dates=[\"timestamp\"])\n",
    "\n",
    "        # Filter rows by year for train and test sets\n",
    "        train_df = merged_df[merged_df[\"timestamp\"].dt.year == 2016]\n",
    "        test_df = merged_df[merged_df[\"timestamp\"].dt.year == 2017]\n",
    "\n",
    "        # Define subfolder paths for train and test data\n",
    "        train_file = output_file.replace(\".CSV\", \"_TRAIN.CSV\")\n",
    "        test_file = output_file.replace(\".CSV\", \"_TEST.CSV\")\n",
    "\n",
    "        # Drop `timestamp` in final train and test outputs\n",
    "        train_df = train_df.drop(columns=[\"timestamp\"])\n",
    "        test_df = test_df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "        # Save train and test dataframes to their respective files\n",
    "        os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "        train_df.to_csv(train_file, index=False)\n",
    "        test_df.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"Train data saved to {train_file}\")\n",
    "        print(f\"Test data saved to {test_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred while splitting data: {e}. Skipping file {output_file}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Process each dataset in DATASET_NAMES and each configuration in data_map\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    if dataset_name in data_map:\n",
    "        for index, config in data_map[dataset_name].items():\n",
    "            output_file = extract_and_merge_data(dataset_name, config)\n",
    "            split_train_test(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T08:53:51.408864Z",
     "start_time": "2024-11-07T08:53:47.726287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_31132/661957321.py:15: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  electricity_df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electricity data processed and saved to ../data/meters/final/ELECTRICITY/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER.CSV\n",
      "Train data saved to ../DATA/METERS/FINAL/ELECTRICITY/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER_TRAIN.CSV\n",
      "Test data saved to ../DATA/METERS/FINAL/ELECTRICITY/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER_TEST.CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_31132/661957321.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.drop(columns=[\"timestamp\"], inplace=True)\n",
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_31132/661957321.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.drop(columns=[\"timestamp\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths and constants\n",
    "TARGET_FILE_ELECTRICITY = \"../data/ELECTRICITY/ELECTRICITY.txt\"\n",
    "OUTPUT_DIR = \"../data/meters/final/ELECTRICITY/\"\n",
    "\n",
    "def process_electricity_data():\n",
    "    # Define file path and output file name\n",
    "    processed_file = \"electricity_global_reactive_power\"\n",
    "    output_file = os.path.join(OUTPUT_DIR, processed_file.upper(), f\"{processed_file}.csv\".upper())\n",
    "\n",
    "    # Load the electricity data with Date and Time combined into a timestamp\n",
    "    try:\n",
    "        electricity_df = pd.read_csv(\n",
    "            TARGET_FILE_ELECTRICITY,\n",
    "            sep=\";\",  # Separator is ';' in the source file\n",
    "            parse_dates=[[0, 1]],  # Combine 'Date' and 'Time' columns\n",
    "            dayfirst=True,  # Use day-first format for dates\n",
    "            na_values=\"?\",  # Handle missing values\n",
    "        )\n",
    "        # Rename columns for easier reference\n",
    "        electricity_df.columns = [\n",
    "            \"timestamp\",\n",
    "            \"Global_active_power\",\n",
    "            \"Global_reactive_power\",\n",
    "            \"Voltage\",\n",
    "            \"Global_intensity\",\n",
    "            \"Sub_metering_1\",\n",
    "            \"Sub_metering_2\",\n",
    "            \"Sub_metering_3\",\n",
    "        ]\n",
    "        \n",
    "        # Convert timestamp column to datetime, handling errors\n",
    "        electricity_df[\"timestamp\"] = pd.to_datetime(\n",
    "            electricity_df[\"timestamp\"], errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # Drop rows with invalid dates\n",
    "        electricity_df.dropna(subset=[\"timestamp\"], inplace=True)\n",
    "\n",
    "        # Define detailed aggregations for `Global_reactive_power`\n",
    "        aggregation_dict = {\n",
    "            \"Global_reactive_power\": [\n",
    "                \"sum\",\n",
    "                \"mean\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"first\",\n",
    "                \"last\",\n",
    "                \"median\",\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Simple sum aggregation for other columns\n",
    "        other_columns = {\n",
    "            \"Voltage\": \"sum\",\n",
    "            \"Global_intensity\": \"sum\",\n",
    "            \"Sub_metering_1\": \"sum\",\n",
    "            \"Sub_metering_2\": \"sum\",\n",
    "            \"Sub_metering_3\": \"sum\",\n",
    "        }\n",
    "        aggregation_dict.update(other_columns)\n",
    "\n",
    "        # Resample the data to daily frequency using specified aggregations\n",
    "        resampled_df = electricity_df.resample(\"D\", on=\"timestamp\").agg(aggregation_dict)\n",
    "\n",
    "        # Rename columns for clarity (specifically for `Global_reactive_power`)\n",
    "        resampled_df.columns = [\n",
    "            \"sum_conso\",\n",
    "            \"mean_conso\",\n",
    "            \"min_conso\",\n",
    "            \"max_conso\",\n",
    "            \"first_conso\",\n",
    "            \"last_conso\",\n",
    "            \"median_conso\",\n",
    "        ] + list(other_columns.keys())\n",
    "\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "\n",
    "        # Add time-related features\n",
    "        resampled_df[\"month\"] = resampled_df[\"timestamp\"].dt.month\n",
    "        resampled_df[\"day_of_week\"] = resampled_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "        # Convert all column names to lowercase\n",
    "        resampled_df.columns = [col.lower() for col in resampled_df.columns]\n",
    "\n",
    "        # Save to the specified output file as .csv with comma separator\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        resampled_df.to_csv(output_file, sep=',', index=False)\n",
    "\n",
    "        print(f\"Electricity data processed and saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}. Skipping electricity data.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Value error: {e}. Skipping electricity data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Skipping electricity data.\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_train_test(output_file):\n",
    "    if output_file is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the processed data with `timestamp` included\n",
    "        merged_df = pd.read_csv(output_file, parse_dates=[\"timestamp\"])\n",
    "\n",
    "        # Filter rows by year for train (2006–2008) and test (2009–2010) sets\n",
    "        train_df = merged_df[merged_df[\"timestamp\"].dt.year.isin([2006, 2007, 2008])]\n",
    "        test_df = merged_df[merged_df[\"timestamp\"].dt.year.isin([2009, 2010])]\n",
    "\n",
    "        # Define subfolder paths for train and test data, ensuring uppercase\n",
    "        train_file = output_file.replace(\".CSV\", \"_TRAIN.CSV\")\n",
    "        test_file = output_file.replace(\".CSV\", \"_TEST.CSV\")\n",
    "\n",
    "        # Drop `timestamp` before saving the final train and test files\n",
    "        train_df.drop(columns=[\"timestamp\"], inplace=True)\n",
    "        test_df.drop(columns=[\"timestamp\"], inplace=True)\n",
    "\n",
    "        # Save train and test dataframes to their respective files with comma separator\n",
    "        os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "        train_df.to_csv(train_file.upper(), sep=',', index=False)\n",
    "        test_df.to_csv(test_file.upper(), sep=',', index=False)\n",
    "\n",
    "        print(f\"Train data saved to {train_file.upper()}\")\n",
    "        print(f\"Test data saved to {test_file.upper()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred while splitting data: {e}. Skipping file {output_file}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the process for ELECTRICITY.txt\n",
    "output_file = process_electricity_data()\n",
    "split_train_test(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Done!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset:\n",
      "       sum_conso mean_conso min_conso max_conso first_conso last_conso  \\\n",
      "count          0          0         0         0           0          0   \n",
      "unique         0          0         0         0           0          0   \n",
      "top          NaN        NaN       NaN       NaN         NaN        NaN   \n",
      "freq         NaN        NaN       NaN       NaN         NaN        NaN   \n",
      "\n",
      "       median_conso voltage global_intensity sub_metering_1 sub_metering_2  \\\n",
      "count             0       0                0              0              0   \n",
      "unique            0       0                0              0              0   \n",
      "top             NaN     NaN              NaN            NaN            NaN   \n",
      "freq            NaN     NaN              NaN            NaN            NaN   \n",
      "\n",
      "       sub_metering_3 month day_of_week  \n",
      "count               0     0           0  \n",
      "unique              0     0           0  \n",
      "top               NaN   NaN         NaN  \n",
      "freq              NaN   NaN         NaN  \n",
      "\n",
      "Additional Information:\n",
      "Number of rows: 0\n",
      "Number of columns: 14\n",
      "\n",
      "Column Types:\n",
      "sum_conso           object\n",
      "mean_conso          object\n",
      "min_conso           object\n",
      "max_conso           object\n",
      "first_conso         object\n",
      "last_conso          object\n",
      "median_conso        object\n",
      "voltage             object\n",
      "global_intensity    object\n",
      "sub_metering_1      object\n",
      "sub_metering_2      object\n",
      "sub_metering_3      object\n",
      "month               object\n",
      "day_of_week         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "#file_path = '../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA.CSV'\n",
    "file_path = '../data/meters/final/ELECTRICITY_GLOBAL_REACTIVE_POWER/ELECTRICITY_GLOBAL_REACTIVE_POWER_TRAIN.csv'\n",
    "\n",
    "try:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display a summary of the DataFrame\n",
    "    print(\"Summary of the dataset:\")\n",
    "    print(data.describe(include='all'))  # Summary statistics for each column\n",
    "\n",
    "    # Additional summary information (optional)\n",
    "    print(\"\\nAdditional Information:\")\n",
    "    print(f\"Number of rows: {data.shape[0]}\")\n",
    "    print(f\"Number of columns: {data.shape[1]}\")\n",
    "    print(\"\\nColumn Types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T08:51:34.657035Z",
     "start_time": "2024-11-07T08:51:34.637841Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [sum_conso, mean_conso, min_conso, max_conso, first_conso, last_conso, median_conso, voltage, global_intensity, sub_metering_1, sub_metering_2, sub_metering_3, month, day_of_week]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sum_conso</th>\n      <th>mean_conso</th>\n      <th>min_conso</th>\n      <th>max_conso</th>\n      <th>first_conso</th>\n      <th>last_conso</th>\n      <th>median_conso</th>\n      <th>voltage</th>\n      <th>global_intensity</th>\n      <th>sub_metering_1</th>\n      <th>sub_metering_2</th>\n      <th>sub_metering_3</th>\n      <th>month</th>\n      <th>day_of_week</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-07T08:51:38.001213Z",
     "start_time": "2024-11-07T08:51:37.996909Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
