{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Aggregation and Feature Extraction\n",
    "\n",
    "This script processes multiple datasets for various utilities (e.g., chilled water, electricity, gas), merging time series data for each and performing daily resampling and aggregation. After merging, additional features are calculated, and train/test datasets are generated for modeling or further analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The script performs the following tasks for each dataset:\n",
    "\n",
    "1. **Merge Data**: Combines consumption data with weather data.\n",
    "2. **Resample and Aggregate**: Resamples to daily frequency, calculates daily aggregates, and extracts time-related features.\n",
    "3. **Split into Train and Test Sets**: Splits the data into 2016 (train) and 2017 (test) datasets.\n",
    "\n",
    "## Code Details\n",
    "\n",
    "### Global Variables\n",
    "\n",
    "- `DATASET_NAMES`: List of utility datasets to process, such as \"chilledwater\" and \"electricity\".\n",
    "- `data_map`: Dictionary containing the site, building, and consumer configurations for each dataset type.\n",
    "- `TARGET_FILE_TEMPLATE`: Template path for the meter data file for each dataset.\n",
    "- `WEATHER_FILE`: Path for the weather data file.\n",
    "- `OUTPUT_DIR`: Directory where the final processed files will be saved.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### 1. `extract_and_merge_data(dataset_name, config)`\n",
    "\n",
    "This function merges the meter and weather data, performs daily resampling, calculates aggregation-based features, and saves the results.\n",
    "\n",
    "- **Parameters**:\n",
    "\n",
    "  - `dataset_name` (str): Name of the dataset (e.g., \"water\").\n",
    "  - `config` (dict): Configuration containing `site_name`, `building_name`, and `consumer_name`.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  1. **File Paths and Columns**: Constructs paths and target columns based on dataset configuration.\n",
    "     - Output paths are organized in subfolders using `dataset_name`, `site_name`, `building_name`, and `consumer_name`, so each file has a dedicated folder.\n",
    "  2. **Data Loading**: Reads the meter and weather data, ensuring only relevant columns are loaded.\n",
    "  3. **Merging**: Merges data on `timestamp` and keeps only relevant timestamps.\n",
    "  4. **Daily Resampling**:\n",
    "     - Resamples on daily frequency (`'D'`) using `sum` for the primary target column and `mean` for weather data.\n",
    "     - Calculates the following features for each day:\n",
    "       - `sum_conso`: Daily sum of consumption.\n",
    "       - `mean_conso`: Daily mean of consumption.\n",
    "       - `min_conso`: Daily minimum consumption.\n",
    "       - `max_conso`: Daily maximum consumption.\n",
    "       - `first_conso`: First recorded consumption value for the day.\n",
    "       - `last_conso`: Last recorded consumption value for the day.\n",
    "       - `median_conso`: Median consumption for the day.\n",
    "       - `month`: Month extracted from `timestamp`.\n",
    "       - `day_of_week`: Day of the week (Monday = 0, Sunday = 6).\n",
    "  5. **Remove Timestamp Column**: After resampling and feature generation, the `timestamp` column is removed from the final processed file.\n",
    "  6. **Error Handling**: Catches and logs any issues with file reading, column mismatches, or merging issues.\n",
    "\n",
    "- **Returns**:\n",
    "  - `output_file` (str): Path to the saved file with the resampled and aggregated data, or `None` if an error occurs.\n",
    "\n",
    "#### 2. `split_train_test(output_file)`\n",
    "\n",
    "This function splits the resampled dataset into train (2016) and test (2017) datasets.\n",
    "\n",
    "- **Parameters**:\n",
    "\n",
    "  - `output_file` (str): Path to the file generated by `extract_and_merge_data`.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "  1. **Load Data**: Reads the `output_file`.\n",
    "  2. **Train/Test Split**:\n",
    "     - Filters rows with `timestamp` in 2016 as the train set.\n",
    "     - Filters rows with `timestamp` in 2017 as the test set.\n",
    "  3. **Save Train and Test Files**:\n",
    "     - Saves the train set as `_TRAIN.CSV` and the test set as `_TEST.CSV` in their respective folders.\n",
    "\n",
    "- **Error Handling**: Catches any issues during the loading or filtering process and logs an error.\n",
    "\n",
    "### Main Execution Loop\n",
    "\n",
    "Iterates over each dataset in `DATASET_NAMES` and each configuration in `data_map`:\n",
    "\n",
    "1. Calls `extract_and_merge_data` to process and save the aggregated data.\n",
    "2. Calls `split_train_test` to split the aggregated data into train and test sets.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "# Example to process each dataset and configuration\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    if dataset_name in data_map:\n",
    "        for index, config in data_map[dataset_name].items():\n",
    "            output_file = extract_and_merge_data(dataset_name, config)\n",
    "            split_train_test(output_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T16:43:28.333935Z",
     "start_time": "2024-11-06T16:43:24.553055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_MOUSE_SCIENCE_MICHEAL/ELECTRICITY_MOUSE_SCIENCE_MICHEAL_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD.CSV\n",
      "Train data saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/GAS_PANTHER_EDUCATION_MOHAMMAD/GAS_PANTHER_EDUCATION_MOHAMMAD_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN.CSV\n",
      "Train data saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/GAS_PANTHER_LODGING_DEAN/GAS_PANTHER_LODGING_DEAN_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA.CSV\n",
      "Train data saved to ../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/HOTWATER_FOX_LODGING_ALANA/HOTWATER_FOX_LODGING_ALANA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO.CSV\n",
      "Train data saved to ../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/HOTWATER_ROBIN_EDUCATION_MARGARITO/HOTWATER_ROBIN_EDUCATION_MARGARITO_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA.CSV\n",
      "Train data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_ALISSA/SOLAR_BOBCAT_EDUCATION_ALISSA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN.CSV\n",
      "Train data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/SOLAR_BOBCAT_EDUCATION_COLEMAN/SOLAR_BOBCAT_EDUCATION_COLEMAN_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA.CSV\n",
      "Train data saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/WATER_PANTHER_LODGING_CORA/WATER_PANTHER_LODGING_CORA_TEST.CSV\n",
      "Data extracted and saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA.CSV\n",
      "Train data saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA_TRAIN.CSV\n",
      "Test data saved to ../data/meters/final/WATER_WOLF_EDUCATION_URSULA/WATER_WOLF_EDUCATION_URSULA_TEST.CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define dataset names and data map with configurations for each dataset type\n",
    "DATASET_NAMES = [\n",
    "    \"electricity\",\n",
    "    \"gas\",\n",
    "    \"hotwater\",\n",
    "    \"solar\",\n",
    "    \"water\",\n",
    "]\n",
    "data_map = {\n",
    "    \"electricity\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Mouse\",\n",
    "            \"building_name\": \"science\",\n",
    "            \"consumer_name\": \"Micheal\",\n",
    "        },\n",
    "        1: {\"site_name\": \"Mouse\", \"building_name\": \"health\", \"consumer_name\": \"Estela\"},\n",
    "    },\n",
    "    \"gas\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Mohammad\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"lodging\",\n",
    "            \"consumer_name\": \"Dean\",\n",
    "        },\n",
    "    },\n",
    "    \"hotwater\": {\n",
    "        0: {\"site_name\": \"Fox\", \"building_name\": \"lodging\", \"consumer_name\": \"Alana\"},\n",
    "        1: {\n",
    "            \"site_name\": \"Robin\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Margarito\",\n",
    "        },\n",
    "    },\n",
    "    \"solar\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Bobcat\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Alissa\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Bobcat\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Coleman\",\n",
    "        },\n",
    "    },\n",
    "    \"water\": {\n",
    "        0: {\n",
    "            \"site_name\": \"Panther\",\n",
    "            \"building_name\": \"lodging\",\n",
    "            \"consumer_name\": \"Cora\",\n",
    "        },\n",
    "        1: {\n",
    "            \"site_name\": \"Wolf\",\n",
    "            \"building_name\": \"education\",\n",
    "            \"consumer_name\": \"Ursula\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "TARGET_FILE_TEMPLATE = \"../data/meters/cleaned/{}_cleaned.csv\"\n",
    "WEATHER_FILE = \"../data/weather/weather.csv\"\n",
    "OUTPUT_DIR = \"../data/meters/final/\"\n",
    "\n",
    "\n",
    "def extract_and_merge_data(dataset_name, config):\n",
    "    site_name = config[\"site_name\"]\n",
    "    building_name = config[\"building_name\"]\n",
    "    consumer_name = config[\"consumer_name\"]\n",
    "\n",
    "    # Define file paths and target/output columns\n",
    "    target_file = TARGET_FILE_TEMPLATE.format(dataset_name)\n",
    "    processed_file = f\"{dataset_name}_{site_name}_{building_name}_{consumer_name}\"\n",
    "    output_file = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        processed_file.upper(),\n",
    "        f\"{processed_file}.csv\".upper(),\n",
    "    )\n",
    "\n",
    "    target_column = f\"{site_name}_{building_name}_{consumer_name}\"\n",
    "    new_target_column = dataset_name.capitalize()\n",
    "\n",
    "    try:\n",
    "        # Load target data (meter readings)\n",
    "        target_df = pd.read_csv(target_file, usecols=[\"timestamp\", target_column])\n",
    "        target_df = target_df.rename(columns={target_column: new_target_column})\n",
    "        target_df[\"timestamp\"] = pd.to_datetime(target_df[\"timestamp\"])\n",
    "\n",
    "        # Load weather data, filter for the specific site, and drop `site_id`\n",
    "        weather_df = pd.read_csv(WEATHER_FILE)\n",
    "        weather_df = weather_df[weather_df[\"site_id\"] == site_name].drop(\n",
    "            columns=[\"site_id\"]\n",
    "        )\n",
    "        weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "\n",
    "        # Merge on timestamp, keeping timestamps from the target data only\n",
    "        merged_df = pd.merge(target_df, weather_df, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "        # Resample the data to daily frequency using sum for consumption and mean for weather data\n",
    "        resampled_df = merged_df.resample(\"D\", on=\"timestamp\").agg(\n",
    "            {\n",
    "                new_target_column: [\n",
    "                    \"sum\",\n",
    "                    \"mean\",\n",
    "                    \"min\",\n",
    "                    \"max\",\n",
    "                    \"first\",\n",
    "                    \"last\",\n",
    "                    \"median\",\n",
    "                ],\n",
    "                **{col: \"mean\" for col in weather_df.columns if col != \"timestamp\"},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Flatten column names after aggregation\n",
    "        resampled_df.columns = [\n",
    "            \"sum_conso\",\n",
    "            \"mean_conso\",\n",
    "            \"min_conso\",\n",
    "            \"max_conso\",\n",
    "            \"first_conso\",\n",
    "            \"last_conso\",\n",
    "            \"median_conso\",\n",
    "        ] + list(weather_df.columns[1:])\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "\n",
    "        # Add time-related features\n",
    "        resampled_df[\"month\"] = resampled_df[\"timestamp\"].dt.month\n",
    "        resampled_df[\"day_of_week\"] = resampled_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "        # Save to the specified output file without removing `timestamp`\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        resampled_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Data extracted and saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"File not found: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\n",
    "            f\"Column error: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred: {e}. Skipping {dataset_name} for {site_name} - {building_name} - {consumer_name}.\"\n",
    "        )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_train_test(output_file):\n",
    "    if output_file is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the merged data with `timestamp` included\n",
    "        merged_df = pd.read_csv(output_file, parse_dates=[\"timestamp\"])\n",
    "\n",
    "        # Filter rows by year for train and test sets\n",
    "        train_df = merged_df[merged_df[\"timestamp\"].dt.year == 2016]\n",
    "        test_df = merged_df[merged_df[\"timestamp\"].dt.year == 2017]\n",
    "\n",
    "        # Define subfolder paths for train and test data\n",
    "        train_file = output_file.replace(\".CSV\", \"_TRAIN.CSV\")\n",
    "        test_file = output_file.replace(\".CSV\", \"_TEST.CSV\")\n",
    "\n",
    "        # Drop `timestamp` in final train and test outputs\n",
    "        train_df = train_df.drop(columns=[\"timestamp\"])\n",
    "        test_df = test_df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "        # Save train and test dataframes to their respective files\n",
    "        os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "        train_df.to_csv(train_file, index=False)\n",
    "        test_df.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"Train data saved to {train_file}\")\n",
    "        print(f\"Test data saved to {test_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred while splitting data: {e}. Skipping file {output_file}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Process each dataset in DATASET_NAMES and each configuration in data_map\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    if dataset_name in data_map:\n",
    "        for index, config in data_map[dataset_name].items():\n",
    "            output_file = extract_and_merge_data(dataset_name, config)\n",
    "            split_train_test(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T16:43:31.845339Z",
     "start_time": "2024-11-06T16:43:28.334958Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_22180/2736359685.py:18: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  electricity_df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electricity data processed and saved to ../data/meters/final/ELECTRICITY/ELECTRICITY_GLOBAL_REACTIVE_POWER.CSV\n",
      "Train data saved to ../data/meters/final/ELECTRICITY/ELECTRICITY_GLOBAL_REACTIVE_POWER_train.csv\n",
      "Test data saved to ../data/meters/final/ELECTRICITY/ELECTRICITY_GLOBAL_REACTIVE_POWER_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_22180/2736359685.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.drop(columns=[\"timestamp\"], inplace=True)\n",
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_22180/2736359685.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.drop(columns=[\"timestamp\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths and constants\n",
    "TARGET_FILE_ELECTRICITY = \"../data/ELECTRICITY/ELECTRICITY.txt\"\n",
    "OUTPUT_DIR = \"../data/meters/final/ELECTRICITY/\"\n",
    "\n",
    "\n",
    "def process_electricity_data():\n",
    "    # Define file path and output file name\n",
    "    processed_file = \"electricity_global_reactive_power\"\n",
    "    output_file = os.path.join(\n",
    "        OUTPUT_DIR, f\"{processed_file}.csv\".upper()\n",
    "    )\n",
    "\n",
    "    # Load the electricity data with Date and Time combined into a timestamp\n",
    "    try:\n",
    "        electricity_df = pd.read_csv(\n",
    "            TARGET_FILE_ELECTRICITY,\n",
    "            sep=\";\",  # Separator is ';'\n",
    "            parse_dates=[[0, 1]],  # Combine 'Date' and 'Time' columns\n",
    "            dayfirst=True,  # Use day-first format for dates\n",
    "            na_values=\"?\",  # Handle missing values\n",
    "        )\n",
    "        electricity_df.columns = [\n",
    "            \"timestamp\",\n",
    "            \"Global_active_power\",\n",
    "            \"Global_reactive_power\",\n",
    "            \"Voltage\",\n",
    "            \"Global_intensity\",\n",
    "            \"Sub_metering_1\",\n",
    "            \"Sub_metering_2\",\n",
    "            \"Sub_metering_3\",\n",
    "        ]\n",
    "        electricity_df[\"timestamp\"] = pd.to_datetime(\n",
    "            electricity_df[\"timestamp\"], errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # Drop rows with invalid dates\n",
    "        electricity_df.dropna(subset=[\"timestamp\"], inplace=True)\n",
    "\n",
    "        # Define detailed aggregations for `Global_reactive_power`\n",
    "        aggregation_dict = {\n",
    "            \"Global_reactive_power\": [\n",
    "                \"sum\",\n",
    "                \"mean\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"first\",\n",
    "                \"last\",\n",
    "                \"median\",\n",
    "            ]  # Detailed aggregations\n",
    "        }\n",
    "\n",
    "        # Simple sum aggregation for other columns\n",
    "        other_columns = {\n",
    "            \"Voltage\": \"sum\",\n",
    "            \"Global_intensity\": \"sum\",\n",
    "            \"Sub_metering_1\": \"sum\",\n",
    "            \"Sub_metering_2\": \"sum\",\n",
    "            \"Sub_metering_3\": \"sum\",\n",
    "        }\n",
    "\n",
    "        # Combine all aggregations into a single dictionary\n",
    "        aggregation_dict.update(other_columns)\n",
    "\n",
    "        # Resample the data to daily frequency using the specified aggregations\n",
    "        resampled_df = electricity_df.resample(\"D\", on=\"timestamp\").agg(\n",
    "            aggregation_dict\n",
    "        )\n",
    "\n",
    "        # Rename columns for `Global_reactive_power` aggregations only\n",
    "        resampled_df.columns = [\n",
    "            \"sum_conso\",\n",
    "            \"mean_conso\",\n",
    "            \"min_conso\",\n",
    "            \"max_conso\",\n",
    "            \"first_conso\",\n",
    "            \"last_conso\",\n",
    "            \"median_conso\",\n",
    "        ] + list(other_columns.keys())\n",
    "\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "\n",
    "        # Add time-related features\n",
    "        resampled_df[\"month\"] = resampled_df[\"timestamp\"].dt.month\n",
    "        resampled_df[\"day_of_week\"] = resampled_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "        # Convert all column names to lowercase\n",
    "        resampled_df.columns = [col.lower() for col in resampled_df.columns]\n",
    "\n",
    "        # Save to the specified output file\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        resampled_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Electricity data processed and saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}. Skipping electricity data.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Value error: {e}. Skipping electricity data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Skipping electricity data.\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_train_test(output_file):\n",
    "    if output_file is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the processed data with `timestamp` included\n",
    "        merged_df = pd.read_csv(output_file, parse_dates=[\"timestamp\"])\n",
    "\n",
    "        # Filter rows by year for train (2006–2008) and test (2009–2010) sets\n",
    "        train_df = merged_df[merged_df[\"timestamp\"].dt.year.isin([2006, 2007, 2008])]\n",
    "        test_df = merged_df[merged_df[\"timestamp\"].dt.year.isin([2009, 2010])]\n",
    "\n",
    "        # Define subfolder paths for train and test data\n",
    "        train_file = output_file.replace(\".CSV\", \"_train.csv\")\n",
    "        test_file = output_file.replace(\".CSV\", \"_test.csv\")\n",
    "\n",
    "        # Drop `timestamp` before saving the final train and test files\n",
    "        train_df.drop(columns=[\"timestamp\"], inplace=True)\n",
    "        test_df.drop(columns=[\"timestamp\"], inplace=True)\n",
    "\n",
    "        # Save train and test dataframes to their respective files\n",
    "        os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "        train_df.to_csv(train_file, index=False)\n",
    "        test_df.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"Train data saved to {train_file}\")\n",
    "        print(f\"Test data saved to {test_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred while splitting data: {e}. Skipping file {output_file}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the process for ELECTRICITY.txt\n",
    "output_file = process_electricity_data()\n",
    "split_train_test(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Done!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset:\n",
      "          sum_conso  mean_conso   min_conso   max_conso  first_conso  \\\n",
      "count    366.000000  366.000000  366.000000  366.000000   366.000000   \n",
      "mean    9263.622917  385.984288  301.911258  488.600001   316.520767   \n",
      "std      998.292054   41.595502   32.141453   89.564894    27.813451   \n",
      "min     6971.000000  290.458333   74.000000  313.000000   135.000000   \n",
      "25%     8293.500000  345.562500  284.000000  391.000000   300.000000   \n",
      "50%     9547.081250  397.795052  296.000000  537.000000   312.000000   \n",
      "75%     9964.000000  415.166667  311.000000  554.000000   325.000000   \n",
      "max    11315.000000  471.458333  465.567000  662.000000   465.567000   \n",
      "\n",
      "       last_conso  median_conso  airTemperature  cloudCoverage  \\\n",
      "count  366.000000    366.000000      366.000000     245.000000   \n",
      "mean   325.056285    362.595495       11.697981       0.119125   \n",
      "std     28.395152     31.356386        5.595295       0.897689   \n",
      "min    274.000000    287.500000        0.175000       0.000000   \n",
      "25%    306.000000    339.500000        7.127083       0.000000   \n",
      "50%    319.000000    367.500000       11.281250       0.000000   \n",
      "75%    336.750000    384.000000       16.465625       0.000000   \n",
      "max    465.567000    465.567000       25.916667       9.000000   \n",
      "\n",
      "       dewTemperature  precipDepth1HR  precipDepth6HR  seaLvlPressure  \\\n",
      "count      366.000000             0.0      364.000000      366.000000   \n",
      "mean         7.634102             NaN        3.737637     1015.972030   \n",
      "std          4.975338             NaN       10.441454       10.508863   \n",
      "min         -4.041667             NaN       -1.000000      982.766667   \n",
      "25%          3.610417             NaN        0.000000     1009.861277   \n",
      "50%          7.793750             NaN        0.000000     1017.501190   \n",
      "75%         11.766667             NaN        1.500000     1022.716576   \n",
      "max         18.866667             NaN       70.000000     1043.795833   \n",
      "\n",
      "       windDirection   windSpeed       month  day_of_week  \n",
      "count     366.000000  366.000000  366.000000   366.000000  \n",
      "mean      195.736213    3.965314    6.513661     3.008197  \n",
      "std        75.431759    1.723696    3.455958     2.000668  \n",
      "min        19.166667    0.962500    1.000000     0.000000  \n",
      "25%       143.437500    2.784375    4.000000     1.000000  \n",
      "50%       216.250000    3.693750    7.000000     3.000000  \n",
      "75%       249.479167    4.794792    9.750000     5.000000  \n",
      "max       352.500000   12.058333   12.000000     6.000000  \n",
      "\n",
      "Additional Information:\n",
      "Number of rows: 366\n",
      "Number of columns: 17\n",
      "\n",
      "Column Types:\n",
      "sum_conso         float64\n",
      "mean_conso        float64\n",
      "min_conso         float64\n",
      "max_conso         float64\n",
      "first_conso       float64\n",
      "last_conso        float64\n",
      "median_conso      float64\n",
      "airTemperature    float64\n",
      "cloudCoverage     float64\n",
      "dewTemperature    float64\n",
      "precipDepth1HR    float64\n",
      "precipDepth6HR    float64\n",
      "seaLvlPressure    float64\n",
      "windDirection     float64\n",
      "windSpeed         float64\n",
      "month               int64\n",
      "day_of_week         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = \"../data/meters/final/ELECTRICITY_MOUSE_HEALTH_ESTELA/ELECTRICITY_MOUSE_HEALTH_ESTELA_TRAIN.CSV\"\n",
    "\n",
    "try:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display a summary of the DataFrame\n",
    "    print(\"Summary of the dataset:\")\n",
    "    print(data.describe(include='all'))  # Summary statistics for each column\n",
    "\n",
    "    # Additional summary information (optional)\n",
    "    print(\"\\nAdditional Information:\")\n",
    "    print(f\"Number of rows: {data.shape[0]}\")\n",
    "    print(f\"Number of columns: {data.shape[1]}\")\n",
    "    print(\"\\nColumn Types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T18:18:50.211443Z",
     "start_time": "2024-11-06T18:18:50.176942Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   sum_conso  mean_conso  min_conso  max_conso  first_conso  last_conso  \\\n0     6971.0  290.458333      135.0      326.0        135.0       282.0   \n1     7257.0  302.375000      282.0      327.0        288.0       301.0   \n2     7435.0  309.791667      296.0      325.0        296.0       302.0   \n3     9074.0  378.083333      278.0      501.0        294.0       302.0   \n4     9514.0  396.416667      288.0      542.0        293.0       301.0   \n\n   median_conso  airTemperature  cloudCoverage  dewTemperature  \\\n0         298.0        5.387500            0.0        3.879167   \n1         304.0        9.783333            NaN        9.183333   \n2         311.5        7.954167            0.0        6.687500   \n3         344.0        7.841667            0.0        6.508333   \n4         355.5        7.904167            NaN        7.058333   \n\n   precipDepth1HR  precipDepth6HR  seaLvlPressure  windDirection  windSpeed  \\\n0             NaN             0.0     1016.941667     116.666667   4.470833   \n1             NaN            15.0      998.087500     166.666667   7.241667   \n2             NaN             6.0      991.837500     187.916667   5.633333   \n3             NaN             0.0      982.766667     177.083333   4.145833   \n4             NaN             0.5      984.712500     175.416667   2.750000   \n\n   month  day_of_week  \n0      1            4  \n1      1            5  \n2      1            6  \n3      1            0  \n4      1            1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sum_conso</th>\n      <th>mean_conso</th>\n      <th>min_conso</th>\n      <th>max_conso</th>\n      <th>first_conso</th>\n      <th>last_conso</th>\n      <th>median_conso</th>\n      <th>airTemperature</th>\n      <th>cloudCoverage</th>\n      <th>dewTemperature</th>\n      <th>precipDepth1HR</th>\n      <th>precipDepth6HR</th>\n      <th>seaLvlPressure</th>\n      <th>windDirection</th>\n      <th>windSpeed</th>\n      <th>month</th>\n      <th>day_of_week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6971.0</td>\n      <td>290.458333</td>\n      <td>135.0</td>\n      <td>326.0</td>\n      <td>135.0</td>\n      <td>282.0</td>\n      <td>298.0</td>\n      <td>5.387500</td>\n      <td>0.0</td>\n      <td>3.879167</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1016.941667</td>\n      <td>116.666667</td>\n      <td>4.470833</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7257.0</td>\n      <td>302.375000</td>\n      <td>282.0</td>\n      <td>327.0</td>\n      <td>288.0</td>\n      <td>301.0</td>\n      <td>304.0</td>\n      <td>9.783333</td>\n      <td>NaN</td>\n      <td>9.183333</td>\n      <td>NaN</td>\n      <td>15.0</td>\n      <td>998.087500</td>\n      <td>166.666667</td>\n      <td>7.241667</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7435.0</td>\n      <td>309.791667</td>\n      <td>296.0</td>\n      <td>325.0</td>\n      <td>296.0</td>\n      <td>302.0</td>\n      <td>311.5</td>\n      <td>7.954167</td>\n      <td>0.0</td>\n      <td>6.687500</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>991.837500</td>\n      <td>187.916667</td>\n      <td>5.633333</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9074.0</td>\n      <td>378.083333</td>\n      <td>278.0</td>\n      <td>501.0</td>\n      <td>294.0</td>\n      <td>302.0</td>\n      <td>344.0</td>\n      <td>7.841667</td>\n      <td>0.0</td>\n      <td>6.508333</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>982.766667</td>\n      <td>177.083333</td>\n      <td>4.145833</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9514.0</td>\n      <td>396.416667</td>\n      <td>288.0</td>\n      <td>542.0</td>\n      <td>293.0</td>\n      <td>301.0</td>\n      <td>355.5</td>\n      <td>7.904167</td>\n      <td>NaN</td>\n      <td>7.058333</td>\n      <td>NaN</td>\n      <td>0.5</td>\n      <td>984.712500</td>\n      <td>175.416667</td>\n      <td>2.750000</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T18:18:52.434771Z",
     "start_time": "2024-11-06T18:18:52.421816Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
