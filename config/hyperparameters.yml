cnn:
  num_filters: [64, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.0001, 0.0001]
  optimizer: ['adam']
  factor: [0.1, 0.5]
  patience: [45, 60]
  dropout: [0.3, 0.5]

lstm:
  hidden_size: [50, 100]
  n_layers: [1, 2, 3]
  learning_rate: [0.0001,0.001]
  optimizer: ['adam']
  factor: [0.1, 0.4]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

gru:
  hidden_size: [50, 100]
  n_layers: [1, 2]
  learning_rate: [0.0001,0.001]
  optimizer: ['adam']
  factor: [0.2, 0.3]
  patience: [30, 40]
  dropout: [0.2, 0.3]

cnn_lstm:
  hidden_size: [50, 100]
  n_layers: [1, 2]
  num_filters: [64, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.0001]
  optimizer: ['adam']
  factor: [0.1, 0.3]
  patience: [40, 50]
  dropout: [0.2]

cnn_gru:
  hidden_size: [50, 100]
  n_layers: [1, 2]
  num_filters: [64, 128]
  kernel_size: [3]
  pool_size: [2, 3]
  learning_rate: [0.00001, 0.0001]
  optimizer: ['adam']
  factor: [0.2, 0.4]
  patience: [40,50]
  dropout: [0.2,0.3]

bigru_cnn_bigru:
  hidden_size: [50, 100]
  n_layers: [1, 2]
  num_filters: [64, 96, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.00001, 0.0001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.1, 0.35]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

bilstm_cnn_bilstm:
  hidden_size: [50, 100]
  n_layers: [1, 2]
  num_filters: [64, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.00001, 0.0001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.15, 0.4]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

cnn_attention_bigru:
  hidden_size: [50, 100, 150]
  n_layers: [1, 2]
  num_filters: [64, 96, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.000001,0.00001, 0.0001, 0.001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.1, 0.3]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

cnn_attention_bilstm:
  hidden_size: [50, 100, 150]
  n_layers: [1, 2]
  num_filters: [64, 96, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.000001,0.00001, 0.0001, 0.001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.1, 0.3]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]
