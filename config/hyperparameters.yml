cnn:
  num_filters: [64, 128]
  cnn_layers: [ 1, 2, 3 ]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.0001, 0.0001]
  optimizer: ['adam']
  factor: [0.1, 0.5]
  patience: [45, 60]
  dropout: [0.3, 0.5]

lstm:
  hidden_size: [50, 100]
  lstm_layers: [ 1, 2, 3 ]
  learning_rate: [0.0001,0.001]
  optimizer: ['adam']
  factor: [0.1, 0.4]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

gru:
  hidden_size: [50, 100]
  gru_layers: [ 1, 2, 3 ]
  learning_rate: [0.0001,0.001]
  optimizer: ['adam']
  factor: [0.2, 0.3]
  patience: [30, 40]
  dropout: [0.2, 0.3]

cnn_lstm:
  hidden_size: [50, 100]
  cnn_layers: [ 1, 2, 3 ]
  lstm_layers: [ 1, 2, 3 ]
  num_filters: [64, 128]
  kernel_size: [3, 5]
  pool_size: [2, 3]
  learning_rate: [0.0001]
  optimizer: ['adam']
  factor: [0.1, 0.3]
  patience: [40, 50]
  dropout: [0.2]

cnn_gru:
  hidden_size: [50, 100]
  cnn_layers: [ 1, 2, 3 ]
  gru_layers: [ 1, 2, 3 ]
  num_filters: [64, 128]
  kernel_size: [3]
  pool_size: [2, 3]
  learning_rate: [0.00001, 0.0001]
  optimizer: ['adam']
  factor: [0.2, 0.4]
  patience: [40,50]
  dropout: [0.2,0.3]

bigru_cnn_bigru:
  hidden_size: [50, 100]
  cnn_layers: [ 1, 2, 3 ]
  bigru1_layers: [ 1, 2, 3 ]
  bigru2_layers: [ 1, 2, 3 ]
  num_filters: [64, 96, 128]
  kernel_size: [ 1, 3 ]
  pool_size: [2, 3]
  learning_rate: [0.00001, 0.0001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.1, 0.35]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

bilstm_cnn_bilstm:
  hidden_size: [50, 100]
  cnn_layers: [ 1, 2, 3 ]
  bilstm1_layers: [ 1, 2, 3 ]
  bilstm2_layers: [ 1, 2, 3 ]
  num_filters: [64, 128]
  kernel_size: [ 1, 3 ]
  pool_size: [2, 3]
  learning_rate: [0.00001, 0.0001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.15, 0.4]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.5]

cnn_attention_bigru:
  hidden_size: [50, 100, 150, 200]
  cnn_layers: [ 1, 2, 3 ]
  am_layers: [ 2, 3, 4 ]
  bigru_layers: [ 1, 2, 3 ]
  num_filters: [64, 96, 128]
  kernel_size: [ 1, 3 ]
  pool_size: [2, 3]
  learning_rate: [0.000001,0.00001, 0.0001, 0.001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.1, 0.3]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.4, 0.5]

cnn_attention_bilstm:
  hidden_size: [50, 100, 150, 200]
  cnn_layers: [ 1, 2, 3 ]
  am_layers: [ 2, 3, 4 ]
  bilstm_layers: [ 1, 2, 3 ]
  num_filters: [64, 96, 128]
  kernel_size: [ 1, 3 ]
  pool_size: [2, 3]
  learning_rate: [0.000001,0.00001, 0.0001, 0.001]
  optimizer: ['adam', 'sgd', 'rmsprop']
  factor: [0.1, 0.3]
  patience: [45, 60]
  dropout: [0.2, 0.3, 0.4, 0.5]
